{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "guilty-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "changing-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/projects/patho2/External_Data/ISIC/'\n",
    "groundtruth = os.path.join(basedir, 'ISIC_2020_Training_GroundTruth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "promotional-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(open(groundtruth, \"r\"), delimiter=\",\")\n",
    "x = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "freelance-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_list = []\n",
    "label_list = []\n",
    "for row in range(1, len(x)):\n",
    "    im_list.append(x[row][0])\n",
    "    label_list.append(int(x[row][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fifteen-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_random = list(range(len(im_list)))\n",
    "random.shuffle(ind_random)\n",
    "train_ind = ind_random[:int(len(ind_random) * 0.8)]\n",
    "valid_ind = ind_random[int(len(ind_random) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "female-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(basedir, 'train.txt'), 'a') as f:\n",
    "    for t_ind in train_ind:\n",
    "        im_bn = im_list[t_ind]\n",
    "        im_name = os.path.join(basedir,'train', '{}.jpg'.format(im_bn))\n",
    "        assert os.path.exists(im_name)\n",
    "        label = label_list[t_ind]\n",
    "        f.write('{};{}\\n'.format(im_name, label))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "banned-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(basedir, 'valid.txt'), 'a') as f:\n",
    "    for v_ind in valid_ind:\n",
    "        im_bn = im_list[v_ind]\n",
    "        im_name = os.path.join(basedir,'train', '{}.jpg'.format(im_bn))\n",
    "        assert os.path.exists(im_name)\n",
    "        label = label_list[v_ind]\n",
    "        f.write('{};{}\\n'.format(im_name, label))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "protecting-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_im = glob.glob(os.path.join(basedir, 'test', '*.jpg'))\n",
    "with open(os.path.join(basedir, 'test.txt'), 'a') as f:\n",
    "    for t_im in test_im:\n",
    "        f.write('{};-1\\n'.format(t_im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "recorded-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(basedir, 'train.txt'), 'r') as f:\n",
    "    image_list = [line.rstrip() for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "suspected-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(basedir, 'train_new.txt'), 'w') as f:\n",
    "#     for line in image_list:\n",
    "#         im_p, label = line.split(';')\n",
    "#         im_type = os.path.basename(os.path.dirname(im_p))\n",
    "#         im_bn = os.path.basename(im_p)\n",
    "#         image = Image.open(im_p)\n",
    "#         crop_size = (4096, 6144)\n",
    "#         paths = []\n",
    "#         for scale in [0.25, 0.5, 0.75]:\n",
    "#             scaled_path = os.path.join(basedir, im_type, 'x{}'.format(scale), im_bn)\n",
    "#             paths.append(scaled_path)\n",
    "#             scaled_size = [int(scale * c_size) for c_size in crop_size]\n",
    "#             resized = F.resize(image, scaled_size, Image.BICUBIC)\n",
    "#             resized.save(scaled_path)\n",
    "#         f.write('{};{};{};{};{}\\n'.format(paths[0], paths[1], paths[2], im_p, label))\n",
    "        \n",
    "# with open(os.path.join(basedir, 'valid.txt'), 'r') as f:\n",
    "#     image_list = [line.rstrip() for line in f]\n",
    "# with open(os.path.join(basedir, 'valid_new.txt'), 'w') as f:\n",
    "#     for line in image_list:\n",
    "#         im_p, label = line.split(';')\n",
    "#         im_type = os.path.basename(os.path.dirname(im_p))\n",
    "#         im_bn = os.path.basename(im_p)\n",
    "#         image = Image.open(im_p)\n",
    "#         crop_size = (4096, 6144)\n",
    "#         paths = []\n",
    "#         for scale in [0.25, 0.5, 0.75]:\n",
    "#             scaled_path = os.path.join(basedir, im_type, 'x{}'.format(scale), im_bn)\n",
    "#             paths.append(scaled_path)\n",
    "#             scaled_size = [int(scale * c_size) for c_size in crop_size]\n",
    "#             resized = F.resize(image, scaled_size, Image.BICUBIC)\n",
    "#             resized.save(scaled_path)\n",
    "#         f.write('{};{};{};{};{}\\n'.format(paths[0], paths[1], paths[2], im_p, label))\n",
    "with open(os.path.join(basedir, 'test.txt'), 'r') as f:\n",
    "    image_list = [line.rstrip() for line in f]\n",
    "with open(os.path.join(basedir, 'test_new.txt'), 'w') as f:\n",
    "    for line in image_list:\n",
    "        im_p, label = line.split(';')\n",
    "        im_type = os.path.basename(os.path.dirname(im_p))\n",
    "        im_bn = os.path.basename(im_p)\n",
    "        image = Image.open(im_p)\n",
    "        crop_size = (4096, 6144)\n",
    "        paths = []\n",
    "        for scale in [0.25, 0.5, 0.75]:\n",
    "            scaled_path = os.path.join(basedir, im_type, 'x{}'.format(scale), im_bn)\n",
    "            paths.append(scaled_path)\n",
    "#             scaled_size = [int(scale * c_size) for c_size in crop_size]\n",
    "#             resized = F.resize(image, scaled_size, Image.BICUBIC)\n",
    "#             resized.save(scaled_path)\n",
    "        for p in paths:\n",
    "            assert os.path.exists(p), '{} does not exist'.format(p)\n",
    "        f.write('{};{};{};{};{}\\n'.format(paths[0], paths[1], paths[2], im_p, label))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rough-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/projects/patho1/melanoma_diagnosis/code/multi-scale_melanoma_diagnosis/')\n",
    "from dataset.transforms import Zooming\n",
    "from torchvision.transforms import Compose\n",
    "zoom_transform = Compose([\n",
    "    Zooming(scale_levels=[0.25, 0.5, 0.75, 1.0], size=(4096, 6144))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(basedir, 'train.txt'), 'r') as f:\n",
    "    image_list = [line.rstrip() for line in f]\n",
    "for line in image_list:\n",
    "    im_p, label = line.split(';')\n",
    "    im_type = os.path.basename(os.path.dirname(im_p))\n",
    "    im_bn = os.path.basename(im_p)\n",
    "    image = Image.open(im_p)\n",
    "    crop_size = (4096, 6144)\n",
    "    im_1x = F.resize(image, crop_size, Image.BICUBIC)\n",
    "    \n",
    "    im_1x.save(im_p)\n",
    "    paths = []\n",
    "    resized = zoom_transform({'image': im_1x, 'mask':None})\n",
    "    for i, scale in enumerate(['0.25_2', '0.5_2', '0.75_2', '1.0_2']):\n",
    "        scaled_path = os.path.join(basedir, im_type, 'x{}'.format(scale), im_bn)\n",
    "        paths.append(scaled_path)\n",
    "#             scaled_size = [int(scale * c_size) for c_size in crop_size]\n",
    "#             resized = F.resize(image, scaled_size, Image.BICUBIC)\n",
    "        resized['image'][i].save(scaled_path)\n",
    "    with open(os.path.join(basedir, 'experiment_txt', 'transform2', 'train.txt'), 'a') as f:\n",
    "        f.write('{};{};{};{};{}\\n'.format(paths[0], paths[1], paths[2], paths[3], label))\n",
    "        \n",
    "        \n",
    "with open(os.path.join(basedir, 'valid.txt'), 'r') as f:\n",
    "    image_list = [line.rstrip() for line in f]\n",
    "for line in image_list:\n",
    "    im_p, label = line.split(';')\n",
    "    im_type = os.path.basename(os.path.dirname(im_p))\n",
    "    im_bn = os.path.basename(im_p)\n",
    "    image = Image.open(im_p)\n",
    "    crop_size = (4096, 6144)\n",
    "    im_1x = F.resize(image, crop_size, Image.BICUBIC)\n",
    "    im_1x.save(im_p)\n",
    "    paths = []\n",
    "    resized = zoom_transform({'image': im_1x, 'mask':None})\n",
    "    for i, scale in enumerate(['0.25_2', '0.5_2', '0.75_2', '1.0_2']):\n",
    "        scaled_path = os.path.join(basedir, im_type, 'x{}'.format(scale), im_bn)\n",
    "        paths.append(scaled_path)\n",
    "#             scaled_size = [int(scale * c_size) for c_size in crop_size]\n",
    "#             resized = F.resize(image, scaled_size, Image.BICUBIC)\n",
    "        resized['image'][i].save(scaled_path)\n",
    "    with open(os.path.join(basedir, 'experiment_txt', 'transform2', 'valid.txt'), 'a') as f:\n",
    "        f.write('{};{};{};{};{}\\n'.format(paths[0], paths[1], paths[2], paths[3], label))\n",
    "        \n",
    "\n",
    "with open(os.path.join(basedir, 'test.txt'), 'r') as f:\n",
    "    image_list = [line.rstrip() for line in f]\n",
    "for line in image_list:\n",
    "    im_p, label = line.split(';')\n",
    "    im_type = os.path.basename(os.path.dirname(im_p))\n",
    "    im_bn = os.path.basename(im_p)\n",
    "    image = Image.open(im_p)\n",
    "    crop_size = (4096, 6144)\n",
    "    im_1x = F.resize(image, crop_size, Image.BICUBIC)\n",
    "    im_1x.save(im_p)\n",
    "    paths = []\n",
    "    resized = zoom_transform({'image': im_1x, 'mask':None})\n",
    "    for i, scale in enumerate(['0.25_2', '0.5_2', '0.75_2', '1.0_2']):\n",
    "        scaled_path = os.path.join(basedir, im_type, 'x{}'.format(scale), im_bn)\n",
    "        paths.append(scaled_path)\n",
    "#             scaled_size = [int(scale * c_size) for c_size in crop_size]\n",
    "#             resized = F.resize(image, scaled_size, Image.BICUBIC)\n",
    "        resized['image'][i].save(scaled_path)\n",
    "    with open(os.path.join(basedir, 'experiment_txt', 'transform2', 'test.txt'), 'a') as f: \n",
    "        f.write('{};{};{};{};{}\\n'.format(paths[0], paths[1], paths[2], paths[3], label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-watch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
