{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hearing-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import pdb\n",
    "import glob\n",
    "import openslide\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import torchgeometry as tgm\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import ImageDraw\n",
    "import sys\n",
    "sys.path.append('/projects/patho2/melanoma_diagnosis/code/data_preprocessing/')\n",
    "from skimage.filters import threshold_multiotsu\n",
    "import skimage.morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "removable-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def threshold_entropy_canny(image):\n",
    "\n",
    "    gray = filter.filter_rgb_to_grayscale(image)\n",
    "    canny = filter.filter_canny(gray, output_type=\"bool\")\n",
    "    entropy = filter.filter_entropy(gray, output_type=\"bool\")\n",
    "    mask = canny + entropy\n",
    "    return mask\n",
    "\n",
    "\n",
    "def threshold_otsu(image):\n",
    "    # return labels [0, 1]\n",
    "    rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "    gray = np.dot(image, rgb_weights)\n",
    "    thresholds = threshold_multiotsu(gray, classes=2)\n",
    "    regions = np.digitize(gray, bins=thresholds)\n",
    "    assert len(np.unique(regions)) == 2, \"number of labels not right\"\n",
    "    #print(len(np.unique(regions)))\n",
    "    return regions==0\n",
    "\n",
    "\n",
    "def get_tissue_mask(image):\n",
    "    mask1 = np.array(threshold_entropy_canny(image), dtype=bool)\n",
    "    mask2 = np.array(threshold_otsu(image), dtype=bool)\n",
    "    mask = np.bitwise_or(mask1, mask2)\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # hole filling\n",
    "    im_out = filter.filter_binary_fill_holes(mask)\n",
    "    mask = np.array(im_out, dtype=np.uint8)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def draw_cnt(cnt, size):\n",
    "    cnt = np.asarray(cnt, dtype=np.float32)\n",
    "    mask = Image.new('L', size)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    draw.polygon(cnt, fill=1, outline=1)\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "careful-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_path = '/projects/patho2/melanoma_diagnosis/x2.5/segmented'\n",
    "originals_directory = '/projects/melanoma/mpath/originals/Set1'\n",
    "output_path = '/projects/patho2/melanoma_diagnosis/x10/segmented_nobg'\n",
    "mask_path = '/projects/melanoma/Meredith/ROI_masks'\n",
    "ratio = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "matched-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse ROI box into dict\n",
    "ROI_dict = {}\n",
    "ROI_csv = '/projects/patho2/melanoma_diagnosis/ROI/ROI_info_expert.csv'\n",
    "with open(ROI_csv, \"r\") as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    header = next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        caseID = int(row[1])\n",
    "        # (x, y, width, height)\n",
    "        ROI_dict[caseID] = (int(row[2])//2, int(row[3])//2, int(row[4])//4, int(row[5])//4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "varying-demand",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'slide_pointer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-26dd41fc8160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenslide\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenSlide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel_dimensions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         assert np.ceil(w / slide_pointer.level_dimensions[4][0]) == ratio, \"{}: size of x40 and x2.5 does not match\".format(\n\u001b[0m\u001b[1;32m     33\u001b[0m         im_p)\n\u001b[1;32m     34\u001b[0m         \u001b[0mslice_level4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslide_pointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslide_pointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel_dimensions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'slide_pointer' is not defined"
     ]
    }
   ],
   "source": [
    "path = coordinates_path\n",
    "new_inpath = originals_directory\n",
    "ratio = ratio\n",
    "\n",
    "for filename in sorted(glob.glob(os.path.join(path, \"*\"))):\n",
    "    #getting coordinates on x2.5 (calculated and saved once)\n",
    "    segment_name = os.path.join(filename, \"segments.csv\")\n",
    "    image_name = filename.split('/')[-1]\n",
    "    category = os.path.basename(os.path.dirname(os.path.dirname(filename)))\n",
    "    #making doirectory for new resolution\n",
    "    new_image_name = image_name.split(\"_x2.5\")[0]\n",
    "    im_id = int(new_image_name.split('_')[1])\n",
    "\n",
    "    imoutput_path = os.path.join(output_path , category , new_image_name)\n",
    "\n",
    "    if not os.path.exists(imoutput_path):\n",
    "        os.makedirs(imoutput_path)\n",
    "\n",
    "    new_image_path = os.path.join(new_inpath , new_image_name +\n",
    "        \".ndpi\")\n",
    "    if not os.path.exists(new_image_path):\n",
    "        new_image_path = glob.glob(os.path.join(new_inpath , new_image_name + \"_*.ndpi\"))[0]\n",
    "    else:\n",
    "        image = openslide.OpenSlide(new_image_path)\n",
    "    if not os.path.exists(new_image_path):\n",
    "        new_image_path = os.path.join(new_inpath , new_image_name +\n",
    "        \"_x10_z0.tif\")\n",
    "        image = Image.open(new_image_path).convert('RGB')\n",
    "    else:\n",
    "        image = openslide.OpenSlide(new_image_path)\n",
    "        w, h = image.level_dimensions[2]\n",
    "        assert np.ceil(w / slide_pointer.level_dimensions[4][0]) == ratio, \"{}: size of x40 and x2.5 does not match\".format(\n",
    "        im_p)\n",
    "        slice_level4 = image.read_region((0, 0), 4, slide_pointer.level_dimensions[4]).convert('RGB')\n",
    "        otsu_mask = get_tissue_mask(np.array(slice_level4))\n",
    "\n",
    "        \n",
    "    #reading coordinates from the saved segment.csv file\n",
    "    with open(segment_name, \"r\") as read_obj:\n",
    "        csv_reader = csv.reader(read_obj)\n",
    "        header = next(csv_reader)\n",
    "        for row in csv_reader:\n",
    "            crop_num = row[1]\n",
    "            #converting resolution 2.5 to the current resolution\n",
    "            #rotating the crop\n",
    "            pt = []\n",
    "            for i in range(2, len(row)):\n",
    "                res = row[i].strip('][').split(', ') \n",
    "                if len(res) < 2:\n",
    "                    break\n",
    "                res = [int(r) * ratio for r in res]\n",
    "                pt.append(res)\n",
    "\n",
    "            first = row[2].strip('][').split(', ')\n",
    "            first = [int(r) * ratio for r in first]\n",
    "            pt.append(first)\n",
    "            pt_np = np.array(pt)\n",
    "                # ROI mask\n",
    "\n",
    "            cnt = np.asarray(pt, dtype=np.float32)\n",
    "            mask = Image.new('L', (slice_level4.size))\n",
    "            draw = ImageDraw.Draw(mask)\n",
    "            draw.polygon(cnt, fill=1, outline=1)\n",
    "            mask = np.bitwise_and(mask, otsu_mask)\n",
    "            mask = mask * 255\n",
    "            \n",
    "            # resize mask to x10\n",
    "            mask = Image.fromarray(mask)\n",
    "            mask = mask.resize((w, h), resample=PIL.Image.Nearest)\n",
    "            \n",
    "\n",
    "            rect = cv2.minAreaRect(cnt)\n",
    "            # the order: bottom left, top left, top right, bottom right\n",
    "            box = cv2.boxPoints(rect)\n",
    "\n",
    "            # get width and height of the detected rectangle\n",
    "            width = ceil(rect[1][0])\n",
    "            height = ceil(rect[1][1])\n",
    "            left_point_x = np.min(box[:, 0])\n",
    "            right_point_x = np.max(box[:, 0])\n",
    "            top_point_y = np.min(box[:, 1])\n",
    "            bottom_point_y = np.max(box[:, 1])\n",
    "\n",
    "            left_point_y = box[:, 1][np.where(box[:, 0] == left_point_x)][0]\n",
    "            right_point_y = box[:, 1][np.where(box[:, 0] == right_point_x)][0]\n",
    "            top_point_x = box[:, 0][np.where(box[:, 1] == top_point_y)][0]\n",
    "            bottom_point_x = box[:, 0][np.where(box[:, 1] == bottom_point_y)][0]\n",
    "\n",
    "\n",
    "            # angle = rect[2]\n",
    "            if rect[2] % 90 != 0: # no rotation\n",
    "                src_pts = np.array([[left_point_x, left_point_y], \n",
    "                                     [top_point_x, top_point_y], \n",
    "                                     [right_point_x, right_point_y],\n",
    "                                     [bottom_point_x, bottom_point_y]])\n",
    "            else:\n",
    "                box = list(box)\n",
    "                src_pts = np.array([list(box[1]), list(box[2]), list(box[3]), list(box[0])])\n",
    "            min_x, min_y = np.ceil(np.min(src_pts, axis=0)).astype(int)\n",
    "            max_x, max_y = np.ceil(np.max(src_pts, axis=0)).astype(int)\n",
    "            \n",
    "                  \n",
    "            \n",
    "            # if min < 0, pad top or left\n",
    "            # if max > limit, pad right or bottom\n",
    "            pad_left = np.ceil(np.abs(min_x)).astype(int) if min_x < 0 else 0\n",
    "            pad_top = np.ceil(np.abs(min_y)).astype(int) if min_y < 0 else 0\n",
    "            pad_bottom = np.ceil(np.abs(max_y)).astype(int) if max_y > h else 0\n",
    "            pad_right = np.ceil(np.abs(max_x)).astype(int) if max_x > w else 0\n",
    "            #left, top, right, bottom\n",
    "#                     partial_image = image.crop((max(0, min_x), max(0, min_y),\n",
    "#                                                 min(w-1, max_x), min(max_y, h-1)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            partial_image = image.read_region((max(0, min_x)*4, max(0, min_y)*4), 1,\n",
    "                                               (min(w-1, max_x)-max(0, min_x),\n",
    "                                                min(h-1, max_y)-max(0, min_y))).convert('RGB')\n",
    "\n",
    "            partial_mask = mask.crop((max(0, min_x), max(0, min_y),\n",
    "                                        min(w-1, max_x), min(max_y, h-1)))\n",
    "#                     partial_image = image[y_min:y_max, x_min:x_max, :]\n",
    "#                     partial_mask = ROI_mask[y_min:y_max, x_min:x_max, :]\n",
    "\n",
    "            #left, top, right and bottom\n",
    "            pad_right += np.abs(min(0, width + pad_left + pad_right - partial_image.size[0]))\n",
    "            pad_bottom += np.abs(min(0, height + pad_bottom + pad_top - partial_image.size[1]))\n",
    "\n",
    "            partial_image = F.pad(partial_image, padding=[pad_left, pad_top, pad_bottom, pad_right],\n",
    "                                 fill=255)\n",
    "            partial_mask = F.pad(partial_mask, padding=[pad_left, pad_top, pad_bottom, pad_right],\n",
    "                                 fill=0)\n",
    "\n",
    "            src_pts = [[src_pt[0]+pad_left-max(min_x, 0), src_pt[1]+pad_top-max(min_y, 0)] for src_pt in src_pts]\n",
    "            # corrdinate of the points in box\n",
    "            # points after the rectangle has been\n",
    "            # straightened\n",
    "        #     src_pts = torch.Tensor(src_pts).unsqueeze(0)\n",
    "            width = np.ceil(np.linalg.norm(np.array(src_pts[0])-np.array(src_pts[1])))\n",
    "            height = np.ceil(np.linalg.norm(np.array(src_pts[1])-np.array(src_pts[2])))\n",
    "            # [top-left, top-right, bottom-right, bottom-left\n",
    "            dst_pts = np.array([[0, 0],\n",
    "                                [width-1, 0], \n",
    "                                [width-1, height-1], \n",
    "                                [0, height-1]], \n",
    "                               dtype=np.float32)\n",
    "\n",
    "#                     M, size = get_rotation_matrix_cnts(cnt)\n",
    "\n",
    "#                     warped_crop = tgm.warp_perspective(partial_image, M, \n",
    "#                         size, border_value=255)\n",
    "#                     warped_mask = tgm.warpPerspective(partial_mask, M, \n",
    "#                         size, border_value=(0, 0, 0))\n",
    "#                     warped_crop = warped_crop.to_pil_image(warped_crop)\n",
    "#                     warped_mask = warped_mask.to_pil_image(warped_mask)\n",
    "            warped_crop = F.perspective(partial_image, src_pts, dst_pts, interpolation=Image.BICUBIC,\n",
    "                                       fill=255)\n",
    "            warped_crop = warped_crop.crop((0, 0, width, height))\n",
    "            warped_mask = F.perspective(partial_mask, src_pts, dst_pts, interpolation=Image.NEAREST,\n",
    "                                       fill=0)\n",
    "            warped_mask = warped_mask.crop((0, 0, width, height))\n",
    "\n",
    "            # #saving the new crops in the new directory\n",
    "            crop_outname = os.path.join(imoutput_path ,\n",
    "                new_image_name + \"_\" + crop_num + \"_original.tif\")\n",
    "            try:\n",
    "                warped_crop.save(crop_outname)\n",
    "            except:\n",
    "                print('downsizing 25%')\n",
    "                newsize = (int(warped_crop.size[0]*0.75), int(warped_crop.size[1] * 0.75))\n",
    "                warped_crop = warped_crop.resize(newsize)\n",
    "                warped_mask = warped_mask.resize(newsize)\n",
    "                warped_crop.save(crop_outname)\n",
    "            mask_outname = os.path.join(imoutput_path ,\n",
    "                new_image_name + \"_\" + crop_num + \"_mask.png\")\n",
    "#                     mask = Image.fromarray(warped_mask)\n",
    "            overlay_outname = os.path.join(imoutput_path ,\n",
    "                new_image_name + \"_\" + crop_num + \".tif\")\n",
    "\n",
    "            warped_mask.save(mask_outname)\n",
    "            \n",
    "            warped_mask = warped_mask.convert('RGB')\n",
    "            overlay = cv2.bitwise_and(warped_crop, warped_mask)\n",
    "            overlay.save(overlay_outname)\n",
    "            print(mask_outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "comic-absence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'segmented'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "statewide-scholarship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_image = np.array(partial_image)\n",
    "partial_image = cv2.circle(partial_image, tuple(np.array(src_pts[0]).astype(int)), 50, (0, 255, 0), -1)\n",
    "partial_image = cv2.circle(partial_image, tuple(np.array(src_pts[1]).astype(int)), 50, (255, 0, 0), -1)\n",
    "partial_image = cv2.circle(partial_image, tuple(np.array(src_pts[2]).astype(int)), 50, (0, 0, 255), -1)\n",
    "partial_image = cv2.circle(partial_image, tuple(np.array(src_pts[3]).astype(int)), 50, (255, 255, 0), -1)\n",
    "\n",
    "partial_image = cv2.cvtColor(partial_image, cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite('/projects/patho2/melanoma_diagnosis/ROI/partial_dots.jpg', partial_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "seasonal-placement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projects/patho2/melanoma_diagnosis/mpath/originals/Set1/MP_0061_*.ndpi'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(new_inpath , new_image_name + \"_*.ndpi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "running-inquiry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32984, 25142)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warped_crop.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "entire-cathedral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24738, 18856)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warped_mask.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "loved-argument",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32984.25, 25142.25)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width*0.75, height*0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-papua",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
