{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55986f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "Image.MAX_IMAGE_PIXELS = 10000000000\n",
    "from torchvision.transforms import functional as F\n",
    "import math\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "base_dir = '/projects/patho2/melanoma_diagnosis/results/2scale_128x4x32x2/7.5_12/posemb_dropout0.25/visual/all/' \n",
    "im_dir = '/projects/patho2/melanoma_diagnosis/x10/split/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5197c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_image_size(img):\n",
    "    if F._is_pil_image(img):\n",
    "        return img.size\n",
    "    elif isinstance(img, torch.Tensor) and img.dim() > 2:\n",
    "        return img.shape[-2:][::-1]\n",
    "    else:\n",
    "        raise TypeError(\"Unexpected type {}\".format(type(img)))\n",
    "\n",
    "\n",
    "class DivideToScales(object):\n",
    "    def __init__(self, scale_levels: list, size=None, interpolation=Image.BICUBIC):\n",
    "        assert len(scale_levels) > 0, \"Atleast 1 scale is required. Got: {}\".format(scale_levels)\n",
    "\n",
    "        self.scale_levels = sorted(scale_levels) #[0.25, 0.5, 0.75, 1.0]\n",
    "        self.num_scales = len(scale_levels)\n",
    "\n",
    "        if size is not None:\n",
    "            self.resize = self.get_sizes(scale_levels=scale_levels, size=size)\n",
    "        else:\n",
    "            self.resize = None\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sizes(scale_levels, size):\n",
    "        resize = dict()\n",
    "        height_1x, width_1x = size\n",
    "        for sc in scale_levels:\n",
    "            scaled_h, scaled_w = int(sc * height_1x), int(sc * width_1x)\n",
    "            # assert height_1x == int(scaled_h / sc), \"Scale is not correct. Got: {} and {}\".format(height_1x,\n",
    "            #                                                                                       int(scaled_h / sc))\n",
    "            # assert width_1x == int(scaled_w / sc), \"Scale is not correct. Got: {} and {}\".format(width_1x,\n",
    "            #                                                                                      int(scaled_w / sc))\n",
    "            resize[sc] = [scaled_h, scaled_w]\n",
    "        return resize\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(image_sizes, crop_zoom):\n",
    "        image_width, image_height = image_sizes\n",
    "        crop_height = int(round(image_height / crop_zoom))\n",
    "        crop_width = int(round(image_width / crop_zoom))\n",
    "        return crop_height, crop_width\n",
    "\n",
    "    def divide_image_to_scales(self, image, mask, scale):\n",
    "        image = F.resize(image, self.resize[scale], self.interpolation)\n",
    "        if mask is not None:\n",
    "             mask = F.resize(mask, self.resize[scale], Image.NEAREST)\n",
    "        return image, mask\n",
    "\n",
    "    def get_scales(self, sample: dict):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "        if self.resize is None:\n",
    "            width, height = _get_image_size(img=image)\n",
    "            self.resize = self.get_sizes(scale_levels=self.scale_levels, size=(height, width))\n",
    "\n",
    "        images, masks = [], [] if mask is not None else None\n",
    "        for i in self.scale_levels:\n",
    "            im, m = self.divide_image_to_scales(image, mask, i)\n",
    "            images.append(im)\n",
    "            if mask is not None:\n",
    "                masks.append(m)\n",
    "\n",
    "        return {'image': images, 'mask': masks}\n",
    "\n",
    "    def __call__(self, sample: dict):\n",
    "        return self.get_scales(sample=sample)\n",
    "\n",
    "\n",
    "def calculate_bbox(img_size, resize2, idx):\n",
    "    \"\"\"\n",
    "    Function that return the bounding box of a word given its index\n",
    "    Args:\n",
    "        ind: int, ind < number of words\n",
    "\n",
    "    Returns:\n",
    "        Bounding box(int[]): [h_low, h_high, w_low, w_high]\n",
    "    \"\"\"\n",
    "    h, w = img_size\n",
    "    c_w = w // resize2\n",
    "    c_h = h // resize2\n",
    "    crop_length = c_w * c_h\n",
    "    assert idx < crop_length, \"Index Out of Bound\"\n",
    "\n",
    "    # [index]: [pad_top, pad_left, pad_right, pad_bottom]\n",
    "    # top= max((idx % c_h) * (resize2), 0)\n",
    "    top = max(math.floor(idx / c_w) * resize2, 0)\n",
    "    # bottom = min(h, (idx % c_h) * resize2+ resize2)\n",
    "    bottom = min(h, math.floor(idx / c_w) * resize2 + resize2)\n",
    "    left = max((idx % c_w) * resize2, 0)\n",
    "    right = min(w, (idx % c_w) * resize2 + resize2)\n",
    "    # left = max(math.floor(idx / c_h) * resize2, 0)\n",
    "    # right = min(w, math.floor(idx / c_h) * resize2 + resize2)\n",
    "\n",
    "    return [top, bottom, left, right]\n",
    "\n",
    "def resize_image_to_k_crops_size(image, n_crops):\n",
    "    w, h = image.size\n",
    "    crop_size_h = int(math.ceil(h / n_crops))\n",
    "    crop_size_w = int(math.ceil(w / n_crops))\n",
    "\n",
    "    # transform crop\n",
    "    # resize crop to fit the crop size\n",
    "    new_h = n_crops * crop_size_h\n",
    "    new_w = n_crops * crop_size_w\n",
    "\n",
    "    image = F.resize(img=image, size=[new_h, new_w], interpolation=Image.BICUBIC)\n",
    "    return image, (crop_size_w, crop_size_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805b9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KCrops(object):\n",
    "    def __init__(self, scale_levels: list, n_crops=[7]):\n",
    "        self.n_crops_h = n_crops\n",
    "        self.n_crops_w = n_crops\n",
    "\n",
    "        self.scales = scale_levels\n",
    "\n",
    "    def divide_image_to_crops(self, image, scale_ind):\n",
    "        # resize image into small crops\n",
    "        channel, h, w = image.shape\n",
    "        crop_size_h = int(math.ceil(h / self.n_crops_h[scale_ind]))\n",
    "        crop_size_w = int(math.ceil(w / self.n_crops_w[scale_ind]))\n",
    "\n",
    "        # transform crop\n",
    "        # resize crop to fit the crop size\n",
    "        new_h = self.n_crops_h[scale_ind] * crop_size_h\n",
    "        new_w = self.n_crops_w[scale_ind] * crop_size_w\n",
    "\n",
    "        # transform to crops\n",
    "        ## Image to BAGS\n",
    "        image = F.resize(img=image, size=[new_h, new_w], interpolation=Image.BICUBIC)\n",
    "        # [C x N_B_H x B_H x W]] --> [C x N_B_H x B_H x N_B_W x B_W]\n",
    "        crops = torch.reshape(image, (channel, self.n_crops_h[scale_ind], crop_size_h,\n",
    "                                      self.n_crops_w[scale_ind], crop_size_w))\n",
    "        # [C x N_B_H x B_H x N_B_W x B_W] --> [C x N_B_H x N_B_W x B_H x B_W]\n",
    "        crops = crops.permute(0, 1, 3, 2, 4)\n",
    "\n",
    "        # '''\n",
    "        # Preserve dimensionality, move to forward loop\n",
    "        # '''\n",
    "        # #[C x N_B_H x N_B_W x B_H x B_W]--> [C x N_B_w * N_B_h x B_H x B_W]\n",
    "        crops = torch.reshape(crops, (channel, self.n_crops_h[scale_ind] * self.n_crops_w[scale_ind],\n",
    "                                      crop_size_h, crop_size_w))\n",
    "        # #[C x N_B_w * N_B_h x B_H x B_W] --> [N_B_w * N_B_h x C x B_H x B_W]\n",
    "        crops = crops.permute(1, 0, 2, 3)\n",
    "        return crops\n",
    "\n",
    "    def __call__(self, sample: dict):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "        images = []\n",
    "        for i, im in enumerate(image):\n",
    "            images.append(self.divide_image_to_crops(im, i))\n",
    "        masks = None\n",
    "        if mask is not None:\n",
    "            masks = []\n",
    "            for i, m in enumerate(mask):\n",
    "                masks.append(self.divide_image_to_crops(m, i))\n",
    "        return {'image': images, 'mask': masks}\n",
    "    \n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n",
    "    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n",
    "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "    if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\n",
    "    or if the numpy.ndarray has dtype = np.uint8\n",
    "    In the other cases, tensors are returned without scaling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, sample: dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "        image_tensor = [F.to_tensor(im) for im in image] if isinstance(image, list) else F.to_tensor(image)\n",
    "        mask_tensor = None\n",
    "        if mask is not None:\n",
    "            mask_tensor = [torch.ByteTensor(np.array(m)).unsqueeze(0) for m in mask] if isinstance(mask, list) else F.to_tensor(mask)\n",
    "\n",
    "        return {'image': image_tensor, 'mask': mask_tensor}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404988ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_border_as_color(crop, rgb=[1, 0, 0], border=10):\n",
    "    # C x w x h\n",
    "    crop[:,:, :border] = torch.Tensor([[[rgb[0]]], [[rgb[1]]], [[rgb[2]]]]).expand((3, crop.shape[1], border))\n",
    "    crop[:, :, -border:] = torch.Tensor([[[rgb[0]]], [[rgb[1]]], [[rgb[2]]]]).expand((3, crop.shape[1], border))\n",
    "    crop[:, :border, :] = torch.Tensor([[[rgb[0]]], [[rgb[1]]], [[rgb[2]]]]).expand((3, border, crop.shape[2]))\n",
    "    crop[:, -border:, :] = torch.Tensor([[[rgb[0]]], [[rgb[1]]], [[rgb[2]]]]).expand((3, border, crop.shape[2]))\n",
    "    return crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee9b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_dir = os.path.join(base_dir, '3', 'MP_0020_x10_z0_0', 'scale_1')\n",
    "im_p = os.path.join(im_dir, '5', 'MP_0020_x10_z0_0.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6719d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract k crops\n",
    "patch_grad = torch.load(os.path.join(case_dir, 'patch_grads.pth'))\n",
    "ind = np.argsort(patch_grad)\n",
    "original = Image.open(im_p).convert('RGB')\n",
    "t = DivideToScales([1.25])\n",
    "sample = t({'image': original, 'mask': None})\n",
    "image_scales = sample['image']\n",
    "image, crop_size = resize_image_to_k_crops_size(image_scales[0], len(patch_grad))\n",
    "t2 = ToTensor()\n",
    "t3= KCrops([1.25], [int(math.sqrt(len(patch_grad)))])\n",
    "sample = t2({'image': image, 'mask': None})\n",
    "image = sample['image']\n",
    "sample = t3({'image': [image], 'mask': None})\n",
    "crops = sample['image'][0]\n",
    "#os.mkdir(os.path.join(case_dir, 'bags'))\n",
    "for i in range(10):\n",
    "    #save_image(crops[ind[-1-i]], os.path.join(case_dir, 'top_bags', 'crop_{}.jpg'.format(i)))\n",
    "    # draw rectangles on top 5\n",
    "    crop = draw_border_as_color(crops[ind[-1-i]])\n",
    "# re-arrange crops\n",
    "orig_from_crops = crops.permute(1, 0, 2, 3)\n",
    "orig_from_crops = orig_from_crops.reshape(-1, int(sqrt(crops)), \n",
    "                                          int(sqrt(crops)), \n",
    "                                          crops.shape[2], crops.shape[3])\n",
    "orig_from_crops = orig_from_crops.permute(0, 1, 3, 2, 4)\n",
    "orig_from_crops = orig_from_crops.reshape(-1,  int(sqrt(crops))*crops.shape[2],\n",
    "                                          int(sqrt(crops))*crops.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81382b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original.save('whole.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78ee2e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([81, 3, 180, 1278])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crops.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48fcb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_from_crops = crops.permute(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "695fe871",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_from_crops = orig_from_crops.reshape(-1, 9, 9, 180, 1278)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07da9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_from_crops = orig_from_crops.permute(0, 1, 3, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "600e8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_from_crops = orig_from_crops.reshape(-1, 9 *180, 9 * 1278)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c8944395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1620, 11502])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_from_crops.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5767a442",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(orig_from_crops, 'test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555bdbed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
